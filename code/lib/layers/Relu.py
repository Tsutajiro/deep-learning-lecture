# -*- coding: utf-8 -*-

## @package     Relu
#  @brief       Relu layer for back propagation
#  @author      tsutaj
#  @date        November 11, 2018

class Relu:
    ## @brief       Define a mask
    #  @note        This mask is used for determining whether it is non-positive or not for forward propagation.
    def __init__(self):
        self.mask = None

    ## @brief       Forward Propagation
    #  @param       x       an NumPy array: input
    #  @return      an NumPy array: output generated by applying Relu to input
    def forward(self, x):
        self.mask = (x <= 0)
        out = x.copy()
        # Fill the element whose value is 0 or less with 0
        out[self.mask] = 0

        return out

    ## @brief       Backward Propagation
    #  @param       dout    an Numpy Array: input
    #  @return      an NumPy array: Relu-applied values
    def backward(self, dout):
        dout[self.mask] = 0
        return dout